"""
MambaGCNBlock Implementation for 3D Pose Estimation
==================================================
Mamba-GCN æ··åˆæ¶æ„çš„æ ¸å¿ƒæ¨¡å—
ä¸‰åˆ†æ”¯èåˆï¼šMamba (æ—¶åº) + GCN (ç©ºé—´) + Attention (åŸºçº¿)
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from .mamba_layer import MambaBranch
from .gcn_layer import GCNBranch


class AttentionBranch(nn.Module):
    """
    Attention åˆ†æ”¯ï¼šä½œä¸ºåŸºçº¿å¯¹æ¯”
    ä½¿ç”¨ç®€åŒ–çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶
    """

    def __init__(self, dim, num_heads=8, dropout=0.1):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads

        assert dim % num_heads == 0, "dim must be divisible by num_heads"

        # Multi-head attention layers
        self.q_proj = nn.Linear(dim, dim, bias=False)
        self.k_proj = nn.Linear(dim, dim, bias=False)
        self.v_proj = nn.Linear(dim, dim, bias=False)
        self.out_proj = nn.Linear(dim, dim, bias=False)

        self.dropout = nn.Dropout(dropout)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x):
        """
        Args:
            x: [B, T, J, C] - batch, time, joints, channels
        Returns:
            y: [B, T, J, C] - attention enhanced features
        """
        B, T, J, C = x.shape
        identity = x

        # Reshape for attention: [B, T, J, C] â†’ [B*J, T, C]
        x_reshaped = x.view(B * J, T, C)

        # Multi-head attention
        q = self.q_proj(x_reshaped).view(
            B * J, T, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x_reshaped).view(
            B * J, T, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x_reshaped).view(
            B * J, T, self.num_heads, self.head_dim).transpose(1, 2)

        # Scaled dot-product attention
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)

        # [B*J, num_heads, T, head_dim]
        attn_output = torch.matmul(attn_weights, v)
        attn_output = attn_output.transpose(
            1, 2).contiguous().view(B * J, T, C)

        # Output projection
        output = self.out_proj(attn_output)

        # Reshape back: [B*J, T, C] â†’ [B, T, J, C]
        output = output.view(B, T, J, C)

        # Residual connection + normalization
        output = self.norm(output + identity)

        return output


class AdaptiveFusion(nn.Module):
    """
    è‡ªé€‚åº”èåˆæ¨¡å—ï¼šåŠ¨æ€èåˆä¸‰ä¸ªåˆ†æ”¯çš„è¾“å‡º
    ä½¿ç”¨å¯å­¦ä¹ çš„æƒé‡è¿›è¡ŒåŠ æƒèåˆ
    """

    def __init__(self, dim, num_branches=3):
        super().__init__()
        self.dim = dim
        self.num_branches = num_branches

        # åˆ†æ”¯æƒé‡ç”Ÿæˆå™¨
        self.weight_generator = nn.Sequential(
            nn.Linear(dim, dim // 4),
            nn.ReLU(),
            nn.Linear(dim // 4, num_branches),
            nn.Softmax(dim=-1)
        )

        # ç‰¹å¾èåˆå±‚
        self.fusion_proj = nn.Linear(dim * num_branches, dim)
        self.norm = nn.LayerNorm(dim)

    def forward(self, branch_outputs, input_features):
        """
        Args:
            branch_outputs: List of [B, T, J, C] from each branch
            input_features: [B, T, J, C] original input for weight generation
        Returns:
            fused_output: [B, T, J, C] fused features
        """
        B, T, J, C = input_features.shape

        # ç”Ÿæˆè‡ªé€‚åº”æƒé‡ï¼šåŸºäºè¾“å…¥ç‰¹å¾çš„å…¨å±€å¹³å‡
        global_feat = input_features.mean(dim=(1, 2))  # [B, C]
        branch_weights = self.weight_generator(
            global_feat)  # [B, num_branches]

        # å †å åˆ†æ”¯è¾“å‡ºï¼š[B, T, J, C, num_branches]
        stacked_outputs = torch.stack(branch_outputs, dim=-1)

        # æ‰©å±•æƒé‡ç»´åº¦ä»¥åŒ¹é… stacked_outputs
        # [B, num_branches] â†’ [B, 1, 1, 1, num_branches]
        weights = branch_weights.view(B, 1, 1, 1, self.num_branches)

        # åŠ æƒèåˆ
        weighted_outputs = stacked_outputs * \
            weights  # [B, T, J, C, num_branches]
        weighted_sum = weighted_outputs.sum(dim=-1)   # [B, T, J, C]

        # æ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        output = self.norm(weighted_sum + input_features)

        return output, branch_weights


class MambaGCNBlock(nn.Module):
    """
    MambaGCN æ ¸å¿ƒæ¨¡å—ï¼šä¸‰åˆ†æ”¯èåˆæ¶æ„

    Architecture:
    Input [B,T,J,C] 
    â”œâ”€â”€ Branch A: Mamba (æ—¶åºå»ºæ¨¡)
    â”œâ”€â”€ Branch B: GCN (ç©ºé—´å…³ç³») 
    â”œâ”€â”€ Branch C: Attention (åŸºçº¿å¯¹æ¯”)
    â””â”€â”€ Adaptive Fusion â†’ Output [B,T,J,C]
    """

    def __init__(self, dim, num_joints=17, use_mamba=True, use_attention=True):
        """
        Args:
            dim: ç‰¹å¾ç»´åº¦
            num_joints: å…³èŠ‚æ•°é‡ (é»˜è®¤17)
            use_mamba: æ˜¯å¦ä½¿ç”¨ Mamba åˆ†æ”¯
            use_attention: æ˜¯å¦ä½¿ç”¨ Attention åˆ†æ”¯
        """
        super().__init__()
        self.dim = dim
        self.num_joints = num_joints
        self.use_mamba = use_mamba
        self.use_attention = use_attention

        # åˆ†æ”¯æ¨¡å—
        self.branches = nn.ModuleDict()
        self.branch_names = []

        # Branch A: Mamba æ—¶åºåˆ†æ”¯
        if use_mamba:
            self.branches['mamba'] = MambaBranch(
                dim, num_joints, use_mamba=True)
            self.branch_names.append('mamba')

        # Branch B: GCN ç©ºé—´åˆ†æ”¯ (å¿…é¡»åŒ…å«)
        self.branches['gcn'] = GCNBranch(dim, num_joints)
        self.branch_names.append('gcn')

        # Branch C: Attention åŸºçº¿åˆ†æ”¯
        if use_attention:
            self.branches['attention'] = AttentionBranch(dim)
            self.branch_names.append('attention')

        # è‡ªé€‚åº”èåˆæ¨¡å—
        num_branches = len(self.branch_names)
        self.fusion = AdaptiveFusion(dim, num_branches)

        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Sequential(
            nn.Linear(dim, dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(dim, dim)
        )

        self.final_norm = nn.LayerNorm(dim)

    def forward(self, x):
        """
        Args:
            x: [B, T, J, C] - input pose features
        Returns:
            output: [B, T, J, C] - enhanced pose features
            info: dict with branch outputs and fusion weights
        """
        identity = x

        # é€šè¿‡å„åˆ†æ”¯å¤„ç†
        branch_outputs = []
        branch_info = {}

        for branch_name in self.branch_names:
            branch_output = self.branches[branch_name](x)
            branch_outputs.append(branch_output)
            branch_info[f'{branch_name}_output'] = branch_output

        # è‡ªé€‚åº”èåˆ
        fused_output, fusion_weights = self.fusion(branch_outputs, x)

        # è¾“å‡ºæŠ•å½±
        projected_output = self.output_proj(fused_output)

        # æœ€ç»ˆæ®‹å·®è¿æ¥ + å±‚å½’ä¸€åŒ–
        final_output = self.final_norm(projected_output + identity)

        # è¿”å›è¯¦ç»†ä¿¡æ¯ç”¨äºåˆ†æ
        info = {
            'branch_info': branch_info,
            'fusion_weights': fusion_weights,
            'fused_output': fused_output,
            'num_branches': len(self.branch_names),
            'branch_names': self.branch_names
        }

        return final_output, info

    def get_model_info(self):
        """è·å–æ¨¡å‹é…ç½®ä¿¡æ¯"""
        return {
            'dim': self.dim,
            'num_joints': self.num_joints,
            'use_mamba': self.use_mamba,
            'use_attention': self.use_attention,
            'branch_names': self.branch_names,
            'num_branches': len(self.branch_names)
        }


def test_mamba_gcn_block():
    """æµ‹è¯•å®Œæ•´çš„ MambaGCNBlock"""
    print("ğŸ§ª æµ‹è¯• MambaGCNBlock...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, time_steps, num_joints, dim = 2, 81, 17, 128
    x = torch.randn(batch_size, time_steps, num_joints, dim)
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {x.shape}")

    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        {"use_mamba": True, "use_attention": True, "name": "å®Œæ•´ä¸‰åˆ†æ”¯"},
        {"use_mamba": True, "use_attention": False, "name": "Mamba+GCN"},
        {"use_mamba": False, "use_attention": True, "name": "GCN+Attention"},
    ]

    for config in configs:
        print(f"\nğŸ”§ æµ‹è¯•é…ç½®: {config['name']}")

        try:
            # åˆ›å»ºæ¨¡å‹
            model = MambaGCNBlock(
                dim=dim,
                num_joints=num_joints,
                use_mamba=config['use_mamba'],
                use_attention=config['use_attention']
            )

            # å‰å‘ä¼ æ’­
            output, info = model(x)

            print(f"âœ… è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"âœ… åˆ†æ”¯æ•°é‡: {info['num_branches']}")
            print(f"âœ… åˆ†æ”¯åç§°: {info['branch_names']}")
            print(f"âœ… èåˆæƒé‡å½¢çŠ¶: {info['fusion_weights'].shape}")

            # æ¢¯åº¦æµ‹è¯•
            loss = output.sum()
            loss.backward()
            print("âœ… æ¢¯åº¦è®¡ç®—æ­£å¸¸")

            # æ¨¡å‹ä¿¡æ¯
            model_info = model.get_model_info()
            print(f"âœ… æ¨¡å‹é…ç½®: {model_info}")

        except Exception as e:
            print(f"âŒ é…ç½® {config['name']} æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    print("\nğŸ‰ MambaGCNBlock æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_mamba_gcn_block()
