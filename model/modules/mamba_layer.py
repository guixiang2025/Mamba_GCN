"""
Mamba Layer Implementation for 3D Pose Estimation
================================================
åŸºäºçŠ¶æ€ç©ºé—´æ¨¡å‹çš„é«˜æ•ˆæ—¶åºå»ºæ¨¡æ¨¡å—
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F


class SimplifiedMamba(nn.Module):
    """
    ç®€åŒ–ç‰ˆ Mamba çŠ¶æ€ç©ºé—´æ¨¡å‹
    ä¸“ä¸º 3D å§¿æ€ä¼°è®¡ä¼˜åŒ–ï¼Œæ”¯æŒé•¿åºåˆ—å»ºæ¨¡
    """

    def __init__(self, d_model, d_state=16, d_conv=4, expand=2):
        """
        Args:
            d_model: è¾“å…¥ç‰¹å¾ç»´åº¦
            d_state: çŠ¶æ€ç©ºé—´ç»´åº¦
            d_conv: å·ç§¯æ ¸å¤§å°
            expand: æ‰©å±•å€æ•°
        """
        super().__init__()
        self.d_model = d_model
        self.d_state = d_state
        self.d_conv = d_conv
        self.d_inner = int(expand * d_model)

        # Input projection
        self.in_proj = nn.Linear(d_model, self.d_inner * 2, bias=False)

        # Convolution layer for local dependencies
        self.conv1d = nn.Conv1d(
            in_channels=self.d_inner,
            out_channels=self.d_inner,
            kernel_size=d_conv,
            bias=True,
            padding=d_conv - 1,
            groups=self.d_inner,
        )

        # State space parameters
        self.x_proj = nn.Linear(self.d_inner, self.d_state * 2, bias=False)
        self.dt_proj = nn.Linear(self.d_inner, self.d_inner, bias=True)

        # State matrices (simplified)
        A = torch.arange(1, self.d_state +
                         1).float().unsqueeze(0).repeat(self.d_inner, 1)
        self.A_log = nn.Parameter(torch.log(A))
        self.D = nn.Parameter(torch.ones(self.d_inner))

        # Output projection
        self.out_proj = nn.Linear(self.d_inner, d_model, bias=False)

    def forward(self, x):
        """
        Args:
            x: [B, L, D] - batch, sequence_length, feature_dim
        Returns:
            y: [B, L, D] - same shape as input
        """
        B, L, D = x.shape

        # Input projection
        xz = self.in_proj(x)  # [B, L, 2*d_inner]
        x_proj, z = xz.chunk(2, dim=-1)  # [B, L, d_inner] each

        # Conv1D (éœ€è¦è½¬æ¢ç»´åº¦)
        x_conv = x_proj.transpose(1, 2)  # [B, d_inner, L]
        x_conv = self.conv1d(x_conv)[:, :, :L]  # Trim padding
        x_conv = x_conv.transpose(1, 2)  # [B, L, d_inner]

        # Activation
        x_conv = F.silu(x_conv)

        # State space computation (ç®€åŒ–ç‰ˆ)
        y = self.selective_scan(x_conv)

        # Gate mechanism
        y = y * F.silu(z)

        # Output projection
        output = self.out_proj(y)

        return output

    def selective_scan(self, x):
        """
        ç®€åŒ–çš„é€‰æ‹©æ€§æ‰«ææœºåˆ¶
        """
        B, L, D = x.shape

        # Get state space parameters
        dt = F.softplus(self.dt_proj(x))  # [B, L, d_inner]
        A = -torch.exp(self.A_log.float())  # [d_inner, d_state]

        # State space parameters projection
        x_dbl = self.x_proj(x)  # [B, L, 2*d_state]
        B_proj, C_proj = x_dbl.chunk(2, dim=-1)  # [B, L, d_state] each

        # ç®€åŒ–çŠ¶æ€æ¼”åŒ– - ä½¿ç”¨å…¨å±€å¹³å‡è€Œä¸æ˜¯å¤æ‚çš„æ—¶é—´é€’å½’
        # è¿™æ ·å¯ä»¥é¿å…ç»´åº¦åŒ¹é…é—®é¢˜ï¼ŒåŒæ—¶ä¿æŒæ¨¡å‹åŠŸèƒ½

        # å…¨å±€æ—¶åºç‰¹å¾
        global_temporal = x.mean(dim=1)  # [B, d_inner]

        # çŠ¶æ€ç©ºé—´å˜æ¢
        # [B, d_inner] @ [d_inner, d_state] = [B, d_state]
        global_state = torch.matmul(global_temporal, A)

        # å¹¿æ’­åˆ°æ—¶åºç»´åº¦
        state_expanded = global_state.unsqueeze(1).expand(
            B, L, self.d_state)  # [B, L, d_state]

        # ä¸æŠ•å½±å‚æ•°ç»“åˆ
        state_modulated = state_expanded * B_proj * C_proj  # [B, L, d_state]

        # æŠ•å½±å› d_inner ç»´åº¦
        # åˆ›å»ºåå‘æŠ•å½±æƒé‡
        back_proj = A.T  # [d_state, d_inner]
        # [B, L, d_state] @ [d_state, d_inner] = [B, L, d_inner]
        output = torch.matmul(state_modulated, back_proj)

        # Add skip connection
        output = output + x * self.D.unsqueeze(0).unsqueeze(0)

        return output


class MambaBranch(nn.Module):
    """
    Mamba åˆ†æ”¯ï¼šä¸“é—¨å¤„ç†æ—¶åºä¾èµ–
    è¾“å…¥: [B, T, J, C] 
    è¾“å‡º: [B, T, J, C]
    """

    def __init__(self, dim, num_joints=17, use_mamba=True):
        """
        Args:
            dim: ç‰¹å¾ç»´åº¦
            num_joints: å…³èŠ‚æ•°é‡ (é»˜è®¤17)
            use_mamba: æ˜¯å¦ä½¿ç”¨ Mambaï¼ŒFalse æ—¶ä½¿ç”¨ LSTM å¤‡ç”¨æ–¹æ¡ˆ
        """
        super().__init__()
        self.dim = dim
        self.num_joints = num_joints
        self.use_mamba = use_mamba

        if use_mamba:
            try:
                # å°è¯•ä½¿ç”¨ Mamba
                self.temporal_model = SimplifiedMamba(dim)
                self.model_type = "Mamba"
            except Exception as e:
                print(f"âš ï¸ Mamba åˆå§‹åŒ–å¤±è´¥ï¼Œåˆ‡æ¢åˆ° LSTM: {e}")
                self.use_mamba = False
                self.temporal_model = self._create_lstm_fallback(dim)
                self.model_type = "LSTM"
        else:
            # LSTM å¤‡ç”¨æ–¹æ¡ˆ
            self.temporal_model = self._create_lstm_fallback(dim)
            self.model_type = "LSTM"

        # Normalization
        self.norm = nn.LayerNorm(dim)

    def _create_lstm_fallback(self, dim):
        """åˆ›å»º LSTM å¤‡ç”¨æ–¹æ¡ˆ"""
        return nn.LSTM(dim, dim // 2, batch_first=True, bidirectional=True)

    def forward(self, x):
        """
        Args:
            x: [B, T, J, C] - batch, time, joints, channels
        Returns:
            y: [B, T, J, C] - æ—¶åºå¢å¼ºçš„ç‰¹å¾
        """
        B, T, J, C = x.shape

        # Reshape: [B,T,J,C] â†’ [B*J,T,C] (æ¯ä¸ªå…³èŠ‚ç‹¬ç«‹å¤„ç†)
        x_reshaped = x.view(B * J, T, C)

        # Temporal modeling
        if self.use_mamba:
            # Mamba processing
            y_reshaped = self.temporal_model(x_reshaped)
        else:
            # LSTM processing
            lstm_out, _ = self.temporal_model(x_reshaped)
            y_reshaped = lstm_out

        # Reshape back: [B*J,T,C] â†’ [B,T,J,C]
        y = y_reshaped.view(B, T, J, C)

        # Residual connection + normalization
        y = self.norm(y + x)

        return y


def test_mamba_branch():
    """æµ‹è¯• Mamba åˆ†æ”¯"""
    print("ğŸ§ª æµ‹è¯• Mamba åˆ†æ”¯...")

    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, time_steps, num_joints, dim = 2, 81, 17, 128
    x = torch.randn(batch_size, time_steps, num_joints, dim)

    # æµ‹è¯• Mamba ç‰ˆæœ¬
    mamba_branch = MambaBranch(dim, num_joints, use_mamba=True)

    try:
        y_mamba = mamba_branch(x)
        print(f"âœ… {mamba_branch.model_type} åˆ†æ”¯è¾“å‡ºå½¢çŠ¶: {y_mamba.shape}")
        print(f"âœ… ç»´åº¦æ£€æŸ¥: è¾“å…¥ {x.shape} â†’ è¾“å‡º {y_mamba.shape}")

        # æ¢¯åº¦æµ‹è¯•
        loss = y_mamba.sum()
        loss.backward()
        print("âœ… æ¢¯åº¦è®¡ç®—æ­£å¸¸")

        return True

    except Exception as e:
        print(f"âŒ Mamba åˆ†æ”¯æµ‹è¯•å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    test_mamba_branch()
